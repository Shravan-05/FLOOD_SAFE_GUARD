{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efd36b-9423-440e-a327-2a452564337e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'flood_risk_dataset_india.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resample\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Sample Data Creation (Increased size for demonstration)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m data =\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mflood_risk_dataset_india.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Create DataFrame\u001b[39;00m\n\u001b[32m     13\u001b[39m df = pd.DataFrame(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'flood_risk_dataset_india.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Sample Data Creation (Increased size for demonstration)\n",
    "data =pd.read_csv('flood_risk_dataset_india.csv')\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df=df.drop(columns=['Land Cover', 'Soil Type','Population Density','Elevation (m)','Infrastructure','River Discharge (m³/s)',], errors='ignore')\n",
    "\n",
    "\n",
    "# Split features and target variable\n",
    "X = df.drop('Flood Occurred', axis=1)\n",
    "y = df['Flood Occurred']\n",
    "\n",
    "# Handle Class Imbalance by Resampling\n",
    "X_0 = X[y == 0]\n",
    "X_1 = X[y == 1]\n",
    "y_0 = y[y == 0]\n",
    "y_1 = y[y == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "X_1_upsampled, y_1_upsampled = resample(X_1,\n",
    "                                         y_1,\n",
    "                                         replace=True,\n",
    "                                         n_samples=len(X_0), \n",
    "                                         random_state=42)\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "X_balanced = pd.concat([X_0, X_1_upsampled])\n",
    "y_balanced = pd.concat([y_0, y_1_upsampled])\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"actual values:\\n \",y_test.values)\n",
    "print(\"predicted : \\n\",y_pred)\n",
    "\n",
    "# Print Classification Report with zero_division handling\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a6872-f253-4659-b4d9-f1e4aa101d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest river level for the user's location is: 85 meters\n",
      "Flood Risk: HIGH 🚨 (Closest Historical Location: 17.42801644751927, 78.5064969511781)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Sample DataFrame (you can replace this with your actual CSV data)\n",
    "data = pd.read_csv('flood_risk_dataset_india.csv')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Haversine Function to calculate distance between two locations\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c  # Distance in km\n",
    "\n",
    "# Predefined river level data for different locations\n",
    "river_levels = {\n",
    "    (17.385044, 78.486671): 85,  # Hyderabad\n",
    "    (18.520407, 73.856255): 95,  # Pune\n",
    "    (19.076090, 72.877426): 80,  # Mumbai\n",
    "    (12.971599, 77.594566): 75   # Bangalore\n",
    "}\n",
    "\n",
    "# Simulate fetching the closest current river level based on user's location\n",
    "def get_current_river_level(lat, lon):\n",
    "    closest_distance = float('inf')  # Start with a very large distance\n",
    "    closest_river_level = None\n",
    "    \n",
    "    # Iterate through predefined river level locations\n",
    "    for (stored_lat, stored_lon), river_level in river_levels.items():\n",
    "        distance = haversine(lat, lon, stored_lat, stored_lon)\n",
    "        \n",
    "        # Find the closest location\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_river_level = river_level\n",
    "    \n",
    "    return closest_river_level\n",
    "\n",
    "# Predict Flood Risk Based on Location and Current River Level\n",
    "def predict_flood_risk(new_lat, new_long, current_river_level, new_data):\n",
    "    # Find the closest historical flood data\n",
    "    closest_location = None\n",
    "    min_distance = float('inf')\n",
    "    threshold = None\n",
    "    \n",
    "    # Iterate through each historical flood location to find the closest one\n",
    "    for index, row in new_data.iterrows():\n",
    "        historical_lat, historical_long = row['Latitude'], row['Longitude']\n",
    "        distance = haversine(new_lat, new_long, historical_lat, historical_long)\n",
    "        \n",
    "        # Find the closest historical flood data point\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_location = row\n",
    "            threshold = row['Water Level (m)']  # Max historical river level for that location\n",
    "    \n",
    "    # If closest historical data is found, compare river levels\n",
    "    if closest_location is not None:\n",
    "        if current_river_level >= threshold:\n",
    "            return f\"Flood Risk: HIGH 🚨 (Closest Historical Location: {closest_location['Latitude']}, {closest_location['Longitude']})\"\n",
    "        else:\n",
    "            return f\"Flood Risk: LOW ✅ (Closest Historical Location: {closest_location['Latitude']}, {closest_location['Longitude']})\"\n",
    "    else:\n",
    "        return \"No historical flood data available for this location.\"\n",
    "\n",
    "# Example: User's location (Hyderabad)\n",
    "new_lat = 17.385044  # User's latitude\n",
    "new_long = 78.486671  # User's longitude\n",
    "\n",
    "# Get the closest river level\n",
    "current_river_level = get_current_river_level(new_lat, new_long)\n",
    "\n",
    "# Output the closest river level\n",
    "print(f\"Closest river level for the user's location is: {current_river_level} meters\")\n",
    "\n",
    "# Make prediction based on current river level and user's location\n",
    "result = predict_flood_risk(new_lat, new_long, current_river_level, df)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7281f98-ab32-465b-b48e-82abb6107647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road: A to B | Status: {'under_flood': True, 'near_flood': False, 'safe': False}\n",
      "Road: B to C | Status: {'under_flood': False, 'near_flood': False, 'safe': True}\n",
      "Road: A to C | Status: {'under_flood': True, 'near_flood': False, 'safe': False}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "river_levels = {\n",
    "    (17.385044, 78.486671): 85,  # Hyderabad (river level 85 meters)\n",
    "    (18.520407, 73.856255): 95,  # Pune (river level 95 meters)\n",
    "    (19.076090, 72.877426): 80,  # Mumbai (river level 80 meters)\n",
    "    (12.971599, 77.594566): 75   # Bangalore (river level 75 meters)\n",
    "}\n",
    "\n",
    "roads = [\n",
    "    ('A to B', (17.385044, 78.486671), (17.395044, 78.496671)),  # Road A to B (Hyderabad)\n",
    "    ('B to C', (17.395044, 78.496671), (17.405044, 78.506671)),  # Road B to C (Hyderabad)\n",
    "    ('A to C', (17.385044, 78.486671), (17.405044, 78.506671))   # Road A to C (Hyderabad)\n",
    "]\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c  # Distance in km\n",
    "\n",
    "def classify_road_flood_status(road_start, road_end, river_levels):\n",
    "    flood_status = {'under_flood': False, 'near_flood': False, 'safe': False}\n",
    "\n",
    "    # Calculate distance from road start and end points to flood-prone areas\n",
    "    for river_location, river_level in river_levels.items():\n",
    "        for point in [road_start, road_end]:\n",
    "            distance = haversine(point[0], point[1], river_location[0], river_location[1])\n",
    "            \n",
    "            # Check if the road is under flood (within 200 meters of a flooded area)\n",
    "            if distance < 0.2:  # 0.2 km = 200 meters\n",
    "                flood_status['under_flood'] = True\n",
    "            # Check if the road is near flood (within 500 meters of a flooded area)\n",
    "            elif distance < 0.5:  # 0.5 km = 500 meters\n",
    "                flood_status['near_flood'] = True\n",
    "    \n",
    "    # If no flood, consider it safe\n",
    "    if not flood_status['under_flood'] and not flood_status['near_flood']:\n",
    "        flood_status['safe'] = True\n",
    "    \n",
    "    return flood_status\n",
    "\n",
    "# Function to check roads and classify their status\n",
    "def check_road_status(roads, river_levels):\n",
    "    road_status = []\n",
    "    \n",
    "    for road_name, road_start, road_end in roads:\n",
    "        status = classify_road_flood_status(road_start, road_end, river_levels)\n",
    "        road_status.append({\n",
    "            'Road': road_name,\n",
    "            'Status': status\n",
    "        })\n",
    "    \n",
    "    return road_status\n",
    "\n",
    "# Get the road status for all roads\n",
    "road_status = check_road_status(roads, river_levels)\n",
    "\n",
    "# Print the road status (whether they are under flood, near flood, or safe)\n",
    "for status in road_status:\n",
    "    print(f\"Road: {status['Road']} | Status: {status['Status']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645328ad-d04d-42ba-91f6-fbcc6c7d2189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66f920-fade-4c0f-b62d-9b50fad77e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
